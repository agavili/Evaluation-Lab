---
title: "Evaluation-Lab"
author: "Anna Stein"
date: "10/27/2021"
output: html_document
---


```{r}
# load in needed packages
install.packages("class")  
library(tidyverse)
library(caret)
library(RColorBrewer)
library(ROCR)
#install.packages("MLmetrics")
library(MLmetrics)
library(class)
```

```{r}
# read in data
music_data = read.csv("/Users/Anna/Downloads/data.csv")
View(music_data)
```


```{r}
# taking out NAs
table(is.na(music_data)) # from this, it seems that there aren't NAs in this dataframe 
music_data <- music_data[complete.cases(music_data), ] # take out rows that contain NAs
View(music_data) # looks like that didn't make a difference


# taking out unecessary columns: column titled 'X', column titled 'target'
music_data = music_data[,-c(1,15)] # don't run this more than once 
View(music_data)


# check variable classes, and change if necessary: 
str(music_data)
# classes seem to be correct, don't need changing

class(music_data$time_signature)
music_data$time_signature = as.character(music_data$time_signature)
class(music_data$time_signature)


numvar_inmusic = names(select_if(music_data, is.numeric))
#View(numvar_inmusic)
# this gives us a table with the columns that are numeric

# scale the numeric variables
music_data[numvar_inmusic] = lapply(music_data[numvar_inmusic], scale)


# normalize numeric variables 
# building a normalizer: 
normalize <- function(x){
 (x - min(x)) / (max(x) - min(x))
}



music_data[numvar_inmusic] = lapply(music_data[numvar_inmusic], normalize)
View(music_data)
# now, all numeric variables are normalized 
```

Need to make danceability into a binary variable: 
```{r}
# need to cut at a certain value for danceability
fivenum(music_data$danceability)
# the median is around 0.6 for danceability, so lets cut there. if danceability is above a 0.6, the song is highly danceable. Otherwise, the song has a low level of danceability. 
music_data$danceability <- cut(music_data$danceability,c(-1,0.6,1),labels = c(0,1))
View(music_data)
# now we have danceability as a binary variable 
```


Finding baseline/prevalence: 
```{r}
(prevalence <- table(music_data$danceability)[[2]]/length(music_data$danceability))
```
Our prevalence is 0.4759544. This will be useful later, in relation to some of our evaluation metrics. 


Question we want to answer with kNN: Can we classify songs based on their danceability (i.e. is the song low-ly or highly danceable?)



Splitting data into train, tune, and test: 
```{r}
# This means that at random, we have an 11.6% chance of correctly picking
# out a subscribed individual. Let's see if kNN can do any better.
# i don't think this is meant to be here??^idk


part_index_1 <- createDataPartition(music_data$danceability,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
View(part_index_1)

train <- music_data[part_index_1,]
tune_and_test <- music_data[-part_index_1, ]

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$danceability,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

dim(train)
dim(tune)
dim(test)
```







Training the classifer: 
```{r}
set.seed(200)
music_7NN<-  knn(train = train[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],#<- training set cases # the variables we want to look at OTHER than danceability (i'm pretty sure)
               test = tune[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],    #<- test set cases
               cl = train[, "danceability"],#<- category for true classification # this would be danceability????
               k = 7,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included
```

Output of training classifier 
```{r}
str(music_7NN)
#table(music_7NN)
#length(music_7NN)
```
This gives us a list of probabilities. 



```{r}
kNN_res = table(music_7NN,
                tune$danceability)
#kNN_res
sum(kNN_res)
```


```{r}
#selecting true negatives and true positives 
kNN_res[row(kNN_res) == col(kNN_res)]

#accuracy
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```
The accuracy here (with k=7) is about 0.623. 

```{r}
confusionMatrix(as.factor(music_7NN), as.factor(tune$danceability), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```
We want to change k for higher accuracy. 

```{r}
# How does "k" affect classification accuracy? Let's create a function
# to calculate classification accuracy based on the number of "k."
# chooseK is our function

chooseK = function(k, train_set, val_set, train_class, val_class){  
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

# this function performs knn for different numbers of neighbors, then calculates the accuracy associated with that number of neigbors (k)
```



```{r}
# The sapply() function plugs in several values into our chooseK function.
#sapply(x, fun...) "fun" here is passing a function to our k-function
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop! Returns a matrix.




knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, # the function that we're using with each value of k
                          train_set = train[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],
                          val_set = tune[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],
                          train_class = train$danceability,
                          val_class = tune$danceability))






# Reformating the results to graph
View(knn_different_k)
class(knn_different_k)#matrix 
head(knn_different_k)

knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
```


Run with optimal k: k = 5

```{r}
set.seed(200)
music_5NN<-  knn(train = train[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],#<- training set cases # the variables we want to look at OTHER than danceability (i'm pretty sure)
               test = tune[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],    #<- test set cases
               cl = train[, "danceability"],#<- category for true classification # this would be danceability????
               k = 5,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included

str(music_5NN)
```

```{r}
kNN_res_5 = table(music_5NN,
                tune$danceability)
#kNN_res
sum(kNN_res_5)

kNN_res[row(kNN_res) == col(kNN_res)]

#accuracy
kNN_acc_5 = sum(kNN_res_5[row(kNN_res_5) == col(kNN_res_5)]) / sum(kNN_res_5)
kNN_acc_5
```
